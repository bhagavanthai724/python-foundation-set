{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyt0TAHu9omaARzofdGyr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhagavanthai724/python-foundation-set/blob/main/14_log__analyser_%26_Incident_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# accepts a raw log file as input\n",
        "from typing import List\n",
        "import sys\n",
        "\n",
        "def read_lines_from_file(path: str) -> List[str]:\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fh:\n",
        "        return [line.rstrip(\"\\n\") for line in fh]\n",
        "\n",
        "def read_lines_from_stdin() -> List[str]:\n",
        "    return [line.rstrip(\"\\n\") for line in sys.stdin]"
      ],
      "metadata": {
        "id": "bNgPdQuFpzGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parses logs\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "_SIMPLE_RE = re.compile(r'^(?P<ts>\\S+)\\s+(?P<level>[A-Za-z]+)\\s+(?P<src>\\S+)\\s*[-:]\\s*(?P<msg>.*)$')\n",
        "\n",
        "def parse_line(line: str) -> Dict[str, Any]:\n",
        "    m = _SIMPLE_RE.match(line.strip())\n",
        "    if m:\n",
        "        return {\n",
        "            \"raw\": line,\n",
        "            \"timestamp_raw\": m.group(\"ts\"),\n",
        "            \"level_raw\": m.group(\"level\"),\n",
        "            \"source_raw\": m.group(\"src\"),\n",
        "            \"message_raw\": m.group(\"msg\").strip(),\n",
        "        }\n",
        "\n",
        "    return {\n",
        "        \"raw\": line,\n",
        "        \"timestamp_raw\": None,\n",
        "        \"level_raw\": None,\n",
        "        \"source_raw\": None,\n",
        "        \"message_raw\": line.strip(),\n",
        "    }\n",
        "\n",
        "def parse_lines(lines: List[str]) -> List[Dict[str, Any]]:\n",
        "    return [parse_line(l) for l in lines if l is not None and l != \"\"]"
      ],
      "metadata": {
        "id": "wovmbisyqJlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizes logs\n",
        "from typing import Dict, Any, List\n",
        "from datetime import datetime\n",
        "\n",
        "_LEVEL_MAP = {\n",
        "    \"debug\": \"DEBUG\",\n",
        "    \"info\": \"INFO\",\n",
        "    \"information\": \"INFO\",\n",
        "    \"warn\": \"WARNING\",\n",
        "    \"warning\": \"WARNING\",\n",
        "    \"error\": \"ERROR\",\n",
        "    \"critical\": \"CRITICAL\",\n",
        "    \"fatal\": \"CRITICAL\",\n",
        "}\n",
        "\n",
        "def normalize_level(raw: str):\n",
        "    if not raw:\n",
        "        return \"INFO\"\n",
        "    return _LEVEL_MAP.get(raw.strip().lower(), raw.strip().upper())\n",
        "\n",
        "def normalize_timestamp(raw_ts):\n",
        "    if not raw_ts:\n",
        "        return None\n",
        "    try:\n",
        "        if raw_ts.endswith(\"Z\"):\n",
        "            return datetime.fromisoformat(raw_ts.replace(\"Z\", \"+00:00\")).isoformat()\n",
        "        return datetime.fromisoformat(raw_ts).isoformat()\n",
        "    except Exception:\n",
        "        return raw_ts\n",
        "\n",
        "def normalize_record(parsed: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"raw\": parsed.get(\"raw\"),\n",
        "        \"timestamp\": normalize_timestamp(parsed.get(\"timestamp_raw\")),\n",
        "        \"level\": normalize_level(parsed.get(\"level_raw\")),\n",
        "        \"source\": parsed.get(\"source_raw\") or \"unknown\",\n",
        "        \"message\": parsed.get(\"message_raw\") or \"\",\n",
        "    }\n",
        "\n",
        "def normalize_all(parsed_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    return [normalize_record(p) for p in parsed_list]"
      ],
      "metadata": {
        "id": "7pb2CDq2qUMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracts structured fields (timestamp, level, message)\n",
        "import re\n",
        "from typing import Dict, Any\n",
        "\n",
        "_KV_RE = re.compile(r'(?P<k>[A-Za-z0-9_\\-]+)=(?P<v>\"[^\"]*\"|\\'[^\\']*\\'|\\S+)')\n",
        "\n",
        "def extract_kv(message: str) -> Dict[str, Any]:\n",
        "    result = {}\n",
        "    for m in _KV_RE.finditer(message or \"\"):\n",
        "        key = m.group(\"k\")\n",
        "        val = m.group(\"v\")\n",
        "        # strip quotes if present\n",
        "        if (val.startswith('\"') and val.endswith('\"')) or (val.startswith(\"'\") and val.endswith(\"'\")):\n",
        "            val = val[1:-1]\n",
        "        result[key] = val\n",
        "    return result\n",
        "\n",
        "def enrich_record(record: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    record = dict(record)  # copy\n",
        "    record.setdefault(\"extracted\", {})\n",
        "    kvs = extract_kv(record.get(\"message\", \"\"))\n",
        "    record[\"extracted\"].update(kvs)\n",
        "    return record"
      ],
      "metadata": {
        "id": "m9ps9kGIrIUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# detects incidents and assigns severity,\n",
        "from typing import Dict, Any, List\n",
        "from collections import defaultdict\n",
        "\n",
        "_HIGH_KEYWORDS = [\"exception\", \"traceback\", \"failed\", \"panic\", \"oom\", \"error\", \"fatal\"]\n",
        "\n",
        "def simple_severity(record: Dict[str, Any]) -> str:\n",
        "    msg = (record.get(\"message\") or \"\").lower()\n",
        "    level = (record.get(\"level\") or \"INFO\").upper()\n",
        "    if level in (\"CRITICAL\", \"FATAL\"):\n",
        "        return \"SEV_CRITICAL\"\n",
        "    if level == \"ERROR\":\n",
        "        return \"SEV_HIGH\"\n",
        "    if level == \"WARNING\":\n",
        "        return \"SEV_MEDIUM\"\n",
        "    for kw in _HIGH_KEYWORDS:\n",
        "        if kw in msg:\n",
        "            return \"SEV_HIGH\"\n",
        "    return \"SEV_LOW\"\n",
        "\n",
        "def detect_incidents(records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    counts = defaultdict(int)\n",
        "    for r in records:\n",
        "        key = (r.get(\"source\"), r.get(\"message\"))\n",
        "        counts[key] += 1\n",
        "\n",
        "    incidents = []\n",
        "    for r in records:\n",
        "        key = (r.get(\"source\"), r.get(\"message\"))\n",
        "        cnt = counts[key]\n",
        "        sev = simple_severity(r)\n",
        "        if cnt >= 3 and sev == \"SEV_LOW\":\n",
        "            sev = \"SEV_MEDIUM\"\n",
        "        incidents.append({\n",
        "            \"timestamp\": r.get(\"timestamp\"),\n",
        "            \"source\": r.get(\"source\"),\n",
        "            \"message\": r.get(\"message\"),\n",
        "            \"count_same\": cnt,\n",
        "            \"severity\": sev,\n",
        "            \"extracted\": r.get(\"extracted\", {})\n",
        "        })\n",
        "    unique = {}\n",
        "    for inc in incidents:\n",
        "        k = (inc[\"source\"], inc[\"message\"])\n",
        "        if k not in unique:\n",
        "            unique[k] = inc\n",
        "    return list(unique.values())"
      ],
      "metadata": {
        "id": "6jR8K92NrXLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finalize Project by integrating Log Analyzer and Incident Detector into a single end-to-end pipeline that:\n",
        "import argparse\n",
        "import json\n",
        "from step1_reader import read_lines_from_file, read_lines_from_stdin\n",
        "from step2_parser import parse_lines\n",
        "from step3_normalizer import normalize_all\n",
        "from step4_extractor import enrich_record\n",
        "from step5_incident_detector import detect_incidents\n",
        "\n",
        "def run_pipeline(lines):\n",
        "    parsed = parse_lines(lines)\n",
        "    normalized = normalize_all(parsed)\n",
        "    enriched = [enrich_record(r) for r in normalized]\n",
        "    incidents = detect_incidents(enriched)\n",
        "    summary = {\n",
        "        \"total_lines\": len(lines),\n",
        "        \"total_parsed\": len(enriched),\n",
        "        \"incidents_count\": len(incidents),\n",
        "        \"incidents\": incidents,\n",
        "        \"examples\": enriched[:10],\n",
        "    }\n",
        "    return summary\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"Simple 6-step log tool (beginner-friendly)\")\n",
        "    ap.add_argument(\"-i\", \"--input\", help=\"Input log file path (omit for stdin)\", default=None)\n",
        "    ap.add_argument(\"-o\", \"--output\", help=\"Output JSON file path (default report.json)\", default=\"report.json\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    if args.input:\n",
        "        lines = read_lines_from_file(args.input)\n",
        "    else:\n",
        "        print(\"Reading from stdin (Ctrl-D to finish)...\")\n",
        "        lines = read_lines_from_stdin()\n",
        "\n",
        "    report = run_pipeline(lines)\n",
        "    with open(args.output, \"w\", encoding=\"utf-8\") as fh:\n",
        "        json.dump(report, fh, indent=2, ensure_ascii=False)\n",
        "    print(f\"Saved JSON report to {args.output}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "iyxIDJeCrywD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates a unified JSON summary report,\n",
        "\"\"\"{\n",
        "  \"title\": \"Project Summary\",\n",
        "  \"date\": \"2025-12-04\",\n",
        "  \"summary\": \"This report gives a quick overview of the project status.\",\n",
        "  \"status\": \"On Track\",\n",
        "  \"key_points\": [\n",
        "    \"Work is progressing as planned.\",\n",
        "    \"No major issues reported.\",\n",
        "    \"Next milestone due next week.\"\n",
        "  ],\n",
        "  \"tasks\": [\n",
        "    {\n",
        "      \"task\": \"Build API\",\n",
        "      \"status\": \"Completed\"\n",
        "    },\n",
        "    {\n",
        "      \"task\": \"Test Features\",\n",
        "      \"status\": \"In Progress\"\n",
        "    },\n",
        "    {\n",
        "      \"task\": \"Prepare Release Notes\",\n",
        "      \"status\": \"Pending\"\n",
        "    }\n",
        "  ],\n",
        "  \"risks\": [\n",
        "    {\n",
        "      \"risk\": \"Server downtime\",\n",
        "      \"level\": \"Low\"\n",
        "    }\n",
        "  ],\n",
        "  \"next_steps\": [\n",
        "    \"Finish testing\",\n",
        "    \"Start deployment\",\n",
        "    \"Update documentation\"\n",
        "  ]\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xuna5qNWsuTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provides a clean CLI entry point for running the entire workflow.\n",
        "\"\"\"\n",
        "WORKFLOW SUMMARY\n",
        "\n",
        "1. Read Input Logs\n",
        "   - Load log lines from a file or stdin.\n",
        "\n",
        "2. Parse Log Entries\n",
        "   - Split each line into timestamp, level, and message fields.\n",
        "\n",
        "3. Normalize Fields\n",
        "   - Convert timestamps to a consistent format.\n",
        "   - Standardize log levels (INFO, WARNING, ERROR, etc.).\n",
        "\n",
        "4. Extract Additional Data\n",
        "   - Look for simple key=value pairs inside the log message.\n",
        "\n",
        "5. Detect Incidents\n",
        "   - Mark errors, warnings, and repeated messages as incidents.\n",
        "   - Assign a simple severity level (LOW, MEDIUM, HIGH).\n",
        "\n",
        "6. Generate Summary Report\n",
        "   - Build one unified JSON report that includes:\n",
        "       • total lines processed\n",
        "       • level counts\n",
        "       • incident list\n",
        "       • example parsed records\n",
        "\n",
        "7. Run Entire Workflow via CLI\n",
        "   - Users run the full pipeline using:\n",
        "       python logtool.py -i input.log -o report.json --pretty\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "fHY1J8_4tWRM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}