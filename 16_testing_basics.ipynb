{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMquydu5hyXI87TFZSfAnrF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhagavanthai724/python-foundation-set/blob/main/16_testing_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L92QfOkPBJxn"
      },
      "outputs": [],
      "source": [
        "# Write a simple function add(a, b) and create a pytest test that checks three different cases.\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "from add import add\n",
        "def test_add_cases():\n",
        "    assert add(1, 2) == 3\n",
        "    assert add(-1, 5) == 4\n",
        "    assert add(0, 0) == 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function that returns the maximum of a list and write a test to ensure it raises ValueError on empty list.\n",
        "def max_list(nums):\n",
        "    if not nums:\n",
        "        raise ValueError(\"Empty list\")\n",
        "    return max(nums)\n",
        "import pytest\n",
        "from maximum import max_list\n",
        "def test_max_list_normal():\n",
        "    assert max_list([1, 3, 2]) == 3\n",
        "def test_max_list_empty():\n",
        "    with pytest.raises(ValueError):\n",
        "        max_list([])"
      ],
      "metadata": {
        "id": "rwi6tU1FBiw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a test that checks whether a function correctly handles None input using pytest.raises.\n",
        "def handle_none(x):\n",
        "    if x is None:\n",
        "        raise TypeError(\"None not allowed\")\n",
        "    return x\n",
        "import pytest\n",
        "from handler import handle_none\n",
        "def test_none_input():\n",
        "    with pytest.raises(TypeError):\n",
        "        handle_none(None)"
      ],
      "metadata": {
        "id": "Sr14IoOlBpwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement a function to normalize text (lowercase + strip). Write tests covering multiple input variations.\n",
        "def normalize(text):\n",
        "    return text.strip().lower()\n",
        "from normalize import normalize\n",
        "def test_normalize_cases():\n",
        "    assert normalize(\" Hello \") == \"hello\"\n",
        "    assert normalize(\"WORLD\") == \"world\"\n",
        "    assert normalize(\"   TeSt   \") == \"test\""
      ],
      "metadata": {
        "id": "p6d8TIpEBv4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a test file where you parametrize inputs and expected outputs for a square(n) function.\n",
        "def square(n):\n",
        "    return n * n\n",
        "import pytest\n",
        "from square import square\n",
        "@pytest.mark.parametrize(\"input,output\", [\n",
        "    (2, 4),\n",
        "    (3, 9),\n",
        "    (-4, 16),\n",
        "])\n",
        "def test_square(input, output):\n",
        "    assert square(input) == output"
      ],
      "metadata": {
        "id": "OI-otQhXB3yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function that divides two numbers; write tests for valid division and ZeroDivisionError.\n",
        "def divide(a, b):\n",
        "    return a / b\n",
        "import pytest\n",
        "from divider import divide\n",
        "def test_divide_normal():\n",
        "    assert divide(10, 2) == 5\n",
        "def test_divide_zero():\n",
        "    with pytest.raises(ZeroDivisionError):\n",
        "        divide(5, 0)"
      ],
      "metadata": {
        "id": "BMZ_EjNUB7yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a pytest test that checks if a dictionary contains required keys using assertions.\n",
        "def has_keys(d):\n",
        "    return all(k in d for k in [\"id\", \"email\"])\n",
        "def test_dict_has_keys():\n",
        "    d = {\"id\": 1, \"email\": \"a@b.com\"}\n",
        "    assert has_keys(d) is True"
      ],
      "metadata": {
        "id": "1Qbr4MEtCEgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a small module with 3 functions and write individual tests for each in a separate test file.\n",
        "def add(a, b): return a + b\n",
        "def sub(a, b): return a - b\n",
        "def mult(a, b): return a * b\n",
        "from module3 import add, sub, mult\n",
        "def test_add():  assert add(2, 3) == 5\n",
        "def test_sub():  assert sub(5, 2) == 3\n",
        "def test_mult(): assert mult(3, 4) == 12"
      ],
      "metadata": {
        "id": "qQgLOce4CIb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a test that ensures a log parser function returns a list of dicts with correct schema.\n",
        "def parse_logs(lines):\n",
        "    return [{\"line\": ln, \"length\": len(ln)} for ln in lines]\n",
        "from parser import parse_logs\n",
        "def test_log_schema():\n",
        "    logs = parse_logs([\"hello\"])\n",
        "    assert isinstance(logs, list)\n",
        "    assert isinstance(logs[0], dict)\n",
        "    assert \"line\" in logs[0]\n",
        "    assert \"length\" in logs[0]"
      ],
      "metadata": {
        "id": "0Qk-eU1fCQpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pytest.approx to test a function that performs floating-point calculations.\n",
        "import math\n",
        "def area(r):\n",
        "    return math.pi * r * r\n",
        "from circle import area\n",
        "import pytest\n",
        "def test_circle_area():\n",
        "    assert area(2) == pytest.approx(12.566, rel=1e-3)"
      ],
      "metadata": {
        "id": "nMkVM9QsCTry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a utility that counts words in a sentence and write tests for empty strings, special chars, and long text.\n",
        "def count_words(s):\n",
        "    return len(s.split())\n",
        "from wordcount import count_words\n",
        "def test_empty():\n",
        "    assert count_words(\"\") == 0\n",
        "def test_special():\n",
        "    assert count_words(\"hi @world !\") == 3\n",
        "def test_long():\n",
        "    text = \"one two three four five\"\n",
        "    assert count_words(text) == 5"
      ],
      "metadata": {
        "id": "c3V-xLKJCb6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a test that verifies that a function opens a file using mocked open() (without touching the filesystem).\n",
        "def read_first_line(path):\n",
        "    with open(path) as f:\n",
        "        return f.readline().strip()\n",
        "from reader import read_first_line\n",
        "from unittest.mock import mock_open, patch\n",
        "def test_read_first_line():\n",
        "    m = mock_open(read_data=\"hello\\nworld\")\n",
        "    with patch(\"builtins.open\", m):\n",
        "        assert read_first_line(\"x.txt\") == \"hello\""
      ],
      "metadata": {
        "id": "KR5aALvyCfJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use pytest fixtures to provide sample input data for a cleaning function.\n",
        "def clean(text):\n",
        "    return text.strip().lower()\n",
        "import pytest\n",
        "from cleaner import clean\n",
        "@pytest.fixture\n",
        "def sample():\n",
        "    return \"  HELLO  \"\n",
        "def test_clean(sample):\n",
        "    assert clean(sample) == \"hello\""
      ],
      "metadata": {
        "id": "6fPCkHhRCljq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a test to ensure your Day 15 CLI improvement function returns correct status codes.\n",
        "def status_code(success=True):\n",
        "    return 0 if success else 1\n",
        "from status import status_code\n",
        "def test_status_success():\n",
        "    assert status_code(True) == 0\n",
        "def test_status_fail():\n",
        "    assert status_code(False) == 1"
      ],
      "metadata": {
        "id": "Z7dXGg8DCtsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a test that checks integration: run a mock log through your analyzer and ensure severity output exists.\n",
        "def analyze(lines):\n",
        "    return {\"severity\": \"high\"} if \"ERROR\" in lines[0] else {\"severity\": \"low\"}\n",
        "from analyzer import analyze\n",
        "def test_integration():\n",
        "    result = analyze([\"ERROR disk failure\"])\n",
        "    assert \"severity\" in result\n",
        "    assert result[\"severity\"] == \"high\""
      ],
      "metadata": {
        "id": "AT7pyZ2SC1cH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}